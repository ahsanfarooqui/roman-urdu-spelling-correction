{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiJMaugcVHQU"
   },
   "source": [
    "# Assignment 4 - CS535 - Natural Language Processing\n",
    "# Name: Muhammad Ahsan Farooqui\n",
    "# Roll Number : 20I-2207\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfmSfnKdVatO"
   },
   "source": [
    "# Running Instructions: \n",
    "* Simply Running all cells should run the whole script. \n",
    "* First Part of the assignment contains all functions that are required to run. \n",
    "* In second part, all functions are called\n",
    "* In third part, a few cases are tested. \n",
    "\n",
    "\n",
    "# Challenges Faced: \n",
    "\n",
    "* The corpus size was huge and it forced me to explore new libraries\n",
    "* The corpus data.txt itself contains many slang words and non-standard spellings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVMb74dTVmqH"
   },
   "source": [
    "# Part 1\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yzxmgpsgLMC",
    "outputId": "3ccfcd6f-c72a-4788-b17c-1f3e85032e03"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCfh1e7FIcwc"
   },
   "source": [
    "## Adding Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5pwUa0I1fdaa",
    "outputId": "9cae9cc8-d8d2-4120-e524-fe3a99256e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iteration-utilities in c:\\programdata\\anaconda3\\lib\\site-packages (0.11.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "!pip install iteration-utilities\n",
    "from iteration_utilities import deepflatten\n",
    "import pickle\n",
    "import iteration_utilities\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import operator\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7BovCC8Ie6h"
   },
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nuZFIWtPfdbC"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "  with open('data.txt', 'r') as f:\n",
    "      passage = f.readlines()\n",
    "\n",
    "  overall_data = passage\n",
    "\n",
    "  with open('misspellings.txt', 'r') as f:\n",
    "      reader = csv.reader(f)\n",
    "      misspellings = list(reader)\n",
    "  f.close()\n",
    "  overall_data2 = [\"# \"+x+\" $\" for x in overall_data]\n",
    "  words_corpus = [re.split(\"\\s\",lst) for lst in overall_data2]\n",
    "  #words_corpus = [x.split() for x in overall_data2]\n",
    "  words_corpus = list(deepflatten(words_corpus,depth=1))\n",
    "  return words_corpus,misspellings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40LvMAbmIl6K"
   },
   "source": [
    "## Calculating Word Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z_VlwfSVMF7J"
   },
   "outputs": [],
   "source": [
    "def get_word_unigrams(words_corpus):\n",
    "  word_unigrams = Counter(words_corpus)\n",
    "  word_unigrams = dict(word_unigrams)\n",
    "  word_unigram_probability = dict()\n",
    "  word_unigram_wordlength = dict()\n",
    "  r = sum(list(word_unigrams.values()))\n",
    "  for i in word_unigrams.keys():\n",
    "    word_unigram_probability[i] = word_unigrams[i]/r\n",
    "    word_unigram_wordlength[i] = len(i)\n",
    "  return word_unigrams,word_unigram_probability,word_unigram_wordlength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPlZpbrBI8ti"
   },
   "source": [
    "## Calculating Character Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uOgj_viOV-1x"
   },
   "outputs": [],
   "source": [
    "def get_character_unigrams(words_corpus):\n",
    "  characters_corpus = list(deepflatten(words_corpus,depth=3))\n",
    "  from collections import Counter\n",
    "  character_counters = Counter(characters_corpus)\n",
    "  character_dict = dict(character_counters)\n",
    "  char_unigrams_dict = dict()\n",
    "  return characters_corpus,character_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4IB_Y-I6kAUk"
   },
   "outputs": [],
   "source": [
    "def get_character_bigrams(characters_corpus):\n",
    "  bigram_counts = Counter(zip(characters_corpus, islice(characters_corpus, 1, None)))\n",
    "  bigram_counts = dict(bigram_counts)\n",
    "  return bigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Bon5huaJZtH"
   },
   "source": [
    "## Now Creating Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99W7jZfrRjTb"
   },
   "source": [
    "### Some functions to help create dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yJMY6qJ9fdbS"
   },
   "outputs": [],
   "source": [
    "def initialize(dict1,id=True):\n",
    "    if(id):\n",
    "      for i in range(97,123):\n",
    "          dict1[chr(i)] = dict()\n",
    "          for k in range(97,123):\n",
    "              dict1[chr(i)][chr(k)] = 0\n",
    "    else:\n",
    "      a = list(range(97,123))\n",
    "      a.append(35)\n",
    "      a.append(36)\n",
    "      for i in a:\n",
    "          dict1[chr(i)] = dict()\n",
    "          #print(chr(i))\n",
    "          for k in a:\n",
    "              dict1[chr(i)][chr(k)] = 0\n",
    "\n",
    "    return dict1\n",
    "\n",
    "def get_dictionaries():\n",
    "    insert = {}\n",
    "    substitute = {}\n",
    "    delete = {}\n",
    "    transpose = {}\n",
    "    insert = initialize(insert,id=False)\n",
    "    substitute = initialize(substitute,id=False)\n",
    "    delete = initialize(delete,id=False)\n",
    "    transpose = initialize(transpose,id=False)\n",
    "    return insert,substitute,delete,transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbkgTlE0Rnx9"
   },
   "source": [
    "### Defining the Transpose, Delete, Substitute and Transpose Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ah6uaj_Rfdb0"
   },
   "outputs": [],
   "source": [
    "def transpose1(correct,incorrect):\n",
    "    b = incorrect\n",
    "    if(len(correct)==len(incorrect)):\n",
    "        for i,j in enumerate(zip(incorrect,correct)):\n",
    "            if(j[0]==j[1]):\n",
    "                continue\n",
    "            else:\n",
    "                a = correct\n",
    "                a = list(a)\n",
    "                tmp = a[i]\n",
    "                if(i<len(correct)-1):\n",
    "                  a[i] = a[i+1]\n",
    "                  a[i+1]=tmp\n",
    "                b = incorrect\n",
    "                a = \"\".join(a)\n",
    "                if(a==b):\n",
    "                  return [(\"transpose\",j[0],j[1])]\n",
    "        return []\n",
    "    else:\n",
    "        return [] \n",
    "\n",
    "def delete1(correct,incorrect):\n",
    "    delete_dict = []\n",
    "    correct = \"#\"+correct+\"$\"\n",
    "    incorrect = \"#\"+incorrect+\"$\"\n",
    "    if(len(correct)>len(incorrect)):\n",
    "        for i in range(len(correct)):\n",
    "            if(correct[0:i]+correct[i+1:]==incorrect):\n",
    "                delete_dict.append((\"delete\",correct[i-1],correct[i]))\n",
    "        return delete_dict\n",
    "    else:\n",
    "        return delete_dict\n",
    "def substitute1(correct,incorrect):\n",
    "    if(len(correct)==len(incorrect)):\n",
    "        b = incorrect\n",
    "        for i,j in enumerate(zip(correct,incorrect)):\n",
    "            if(j[0]==j[1]):\n",
    "                continue\n",
    "            else:\n",
    "              a = list(correct)\n",
    "              \n",
    "              a[i] = j[1]\n",
    "              a = \"\".join(a)\n",
    "              if(a==b):\n",
    "                return [(\"substitute\",j[1],j[0])]\n",
    "        return []\n",
    "    else:\n",
    "        return []\n",
    "def insert1(correct,incorrect):\n",
    "    insert_list = []\n",
    "    correct = \"#\"+correct+\"$\"\n",
    "    incorrect = \"#\"+incorrect+\"$\"\n",
    "    if(len(incorrect)>len(correct)):\n",
    "        for i in range(len(incorrect)):\n",
    "            if(incorrect[0:i]+incorrect[i+1:]==correct):\n",
    "                insert_list.append((\"insertion\",correct[i-1],incorrect[i]))\n",
    "        return insert_list\n",
    "    else:\n",
    "        return insert_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('substitute', 'p', 'o')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_operations(\"hello\",\"hellp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAlPPmcARthk"
   },
   "source": [
    "### Returning operations i.e. the insert, correct, delete and substitute against two strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D7Fkf2Gafdb1"
   },
   "outputs": [],
   "source": [
    "def get_operations(correct,incorrect):\n",
    "    if transpose1(correct,incorrect)!=[]:\n",
    "        return transpose1(correct,incorrect)\n",
    "    elif delete1(correct,incorrect)!=[]:\n",
    "        return (delete1(correct,incorrect))\n",
    "    elif substitute1(correct,incorrect)!=[]:\n",
    "        return (substitute1(correct,incorrect)) \n",
    "    elif insert1(correct,incorrect)!=[]:\n",
    "        return (insert1(correct,incorrect))\n",
    "    else:\n",
    "        return [(\"none\",0,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXftRZJkRzwC"
   },
   "source": [
    "### Function that takes into account the misspellings and calculates the error functions. \n",
    "\n",
    "* Note: it is trained on the given misspellings file that has first line equal to [Correct, Incorrect] which is omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r4JGDydkfdb6"
   },
   "outputs": [],
   "source": [
    "def generate_matrix(misspellings):\n",
    "  insert,substitute,delete,transpose = get_dictionaries()\n",
    "  for i in misspellings[1:]:\n",
    "      word = i[0]\n",
    "      others=i[1].split()\n",
    "      others = [a.strip() for a in others]\n",
    "      if(len(others)==1):\n",
    "        others = list(others)\n",
    "      for j in others:\n",
    "          get_opss = get_operations(word,j)\n",
    "          for get_ops in get_opss: \n",
    "            if(get_ops[0]==\"insertion\"):\n",
    "              insert[get_ops[1]][get_ops[2]] = insert[get_ops[1]][get_ops[2]] + 1\n",
    "            elif(get_ops[0]==\"delete\"):\n",
    "              delete[get_ops[1]][get_ops[2]] = delete[get_ops[1]][get_ops[2]] + 1\n",
    "            elif(get_ops[0]==\"substitute\"):\n",
    "              substitute[get_ops[1]][get_ops[2]] = substitute[get_ops[1]][get_ops[2]] + 1\n",
    "            elif(get_ops[0]==\"transpose\"):\n",
    "              transpose[get_ops[1]][get_ops[2]] = transpose[get_ops[1]][get_ops[2]] + 1\n",
    "            \n",
    "  return insert,substitute,delete,transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vul_aaYTSG78"
   },
   "source": [
    "### Once we get the matrix values, dividing it by the character unigrams or bigrams as mentioned in paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mobyFG_pNe5O"
   },
   "outputs": [],
   "source": [
    "def calculate_matrix_probabilities(insert,substitute,delete,transpose,char_unigrams_dict,character_bigram_counts):\n",
    "  for key in insert.keys():\n",
    "    for key2 in insert[key]:\n",
    "      if(insert[key][key2])>0:\n",
    "        insert[key][key2] = insert[key][key2]/char_unigrams_dict[key]\n",
    "  for key in substitute:\n",
    "    for key2 in substitute[key]:\n",
    "      if(substitute[key][key2]>0):\n",
    "        substitute[key][key2] =substitute[key][key2]/char_unigrams_dict[key2]\n",
    "  for key in delete:\n",
    "    for key2 in delete[key]:\n",
    "      if(delete[key][key2]>0):\n",
    "        delete[key][key2] = delete[key][key2]/character_bigram_counts[(key,key2)]\n",
    "  for key in transpose:\n",
    "    for key2 in transpose[key]:\n",
    "      if(transpose[key][key2]>0):\n",
    "        transpose[key][key2] = transpose[key][key2]/character_bigram_counts[(key,key2)]\n",
    "  return insert,substitute,delete,transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvQRhUV0V9CK"
   },
   "source": [
    "## Get Optimum words. \n",
    "* this function gets words unigrams, the word iteself and finds out the best word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SGS74CdSN8_f"
   },
   "outputs": [],
   "source": [
    "def get_optimum_words(word,word_unigrams,word_unigram_wordlength):\n",
    "  shortlisted_words = { your_key: word_unigram_wordlength[your_key] for your_key in word_unigram_wordlength.keys() \n",
    "  if((len(your_key) in [len(word)-2,len(word)-1,len(word),len(word)+1,len(word)+2])\n",
    "    and (get_operations(your_key,word)[0][0]!='none')) }\n",
    "  ops =[]\n",
    "  if(word in word_unigram_wordlength.keys()):\n",
    "    return word,ops\n",
    "  for i in shortlisted_words.keys():\n",
    "    ops.append(get_operations(i,word))\n",
    "  if(len(shortlisted_words)==0):\n",
    "    return word,ops\n",
    "  return shortlisted_words,ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANlqGCvdWECm"
   },
   "source": [
    "## The below function takes all candidate words and calculates the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "a0excguSpp3_"
   },
   "outputs": [],
   "source": [
    "def get_final_word(optimum_words,your_word,insert,substitute,delete,transpose,word_unigram_probability):\n",
    "  score = dict()\n",
    "  for i in optimum_words.keys():\n",
    "    ops = get_operations(i,your_word)\n",
    "    a = 0\n",
    "    for j in ops:\n",
    "      if(j[0]==\"insertion\"):\n",
    "        if(i in score.keys()):\n",
    "          thiscalculate = insert[j[1]][j[2]] * word_unigram_probability[i]\n",
    "          if(thiscalculate>score[i]):\n",
    "            score[i]=thiscalculate\n",
    "        else:\n",
    "          score[i] = insert[j[1]][j[2]] * word_unigram_probability[i]\n",
    "\n",
    "      elif(j[0]==\"substitute\"):\n",
    "        if(i in score.keys()):\n",
    "          thiscalculate = substitute[j[1]][j[2]] * word_unigram_probability[i]\n",
    "          if(thiscalculate>score[i]):\n",
    "            score[i]=thiscalculate\n",
    "        else:\n",
    "          score[i] = substitute[j[1]][j[2]] * word_unigram_probability[i]\n",
    "\n",
    "      elif(j[0]==\"delete\"):\n",
    "        if(i in score.keys()):\n",
    "          thiscalculate = delete[j[1]][j[2]] * word_unigram_probability[i]\n",
    "          if(thiscalculate>score[i]):\n",
    "            score[i]=thiscalculate\n",
    "        else:\n",
    "          score[i] = delete[j[1]][j[2]] * word_unigram_probability[i]\n",
    "\n",
    "      elif(j[0]==\"transpose\"):\n",
    "        if(i in score.keys()):\n",
    "          thiscalculate = transpose[j[1]][j[2]] * word_unigram_probability[i]\n",
    "          if(thiscalculate>score[i]):\n",
    "            score[i]=thiscalculate\n",
    "        else:\n",
    "          score[i] = transpose[j[1]][j[2]] * word_unigram_probability[i]\n",
    "  return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOGLd97jWgkX"
   },
   "source": [
    "## This is the Final Function that gets a word and returns an optimum word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CcB1m0WQlqKx"
   },
   "outputs": [],
   "source": [
    "def spell_checker(word):\n",
    "  word = word.lower()\n",
    "  try:\n",
    "    shortlisted_words,ops = get_optimum_words(word,word_unigrams,word_unigram_wordlength)\n",
    "  except:\n",
    "    return word\n",
    "  try:\n",
    "    a = shortlisted_words.keys()\n",
    "  except:\n",
    "    return word\n",
    "  try:\n",
    "    del shortlisted_words[\"#\"]\n",
    "    del shortlisted_words[\"$\"]\n",
    "    del shortlisted_words[\"\"]\n",
    "  except:\n",
    "    pass\n",
    "  spell_check=get_final_word(shortlisted_words,word,insert,substitute,delete,transpose,word_unigram_probability)\n",
    "  if(len(spell_check)==0):\n",
    "    return word\n",
    "  #final_dict = OrderedDict(sorted(spell_check.items(), key=operator.itemgetter(1),reverse=True))\n",
    "  return max(spell_check.items(), key=operator.itemgetter(1))[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpKfX7lgWqg5"
   },
   "source": [
    "# Part 2\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnKRJi3ZJmET"
   },
   "source": [
    "## Putting it all together!\n",
    "\n",
    "* First line is getting data and also splitting sentences to words\n",
    "* Second line is getting word level unigrams\n",
    "* Third line is getting character unigrams\n",
    "* Fourth is getting character bigrams\n",
    "* Fifth is getting the error matrices \n",
    "* Sixth is calculating the matrix probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XbCI2X86KcXQ"
   },
   "outputs": [],
   "source": [
    "words_corpus,misspellings = get_data()\n",
    "word_unigrams,word_unigram_probability,word_unigram_wordlength = get_word_unigrams(words_corpus)\n",
    "characters_corpus,char_unigrams_dict = get_character_unigrams(words_corpus)\n",
    "character_bigram_counts = get_character_bigrams(characters_corpus)\n",
    "insert,substitute,delete,transpose = generate_matrix(misspellings)\n",
    "insert_p,substitute_p,delete_p,transpose_p = calculate_matrix_probabilities(insert,substitute,delete,transpose,char_unigrams_dict,character_bigram_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeUw5lrHWw57"
   },
   "source": [
    "# Part 3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ts_OI9Kh4R2P"
   },
   "source": [
    "## Now Testing on some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9lzleNG-s5yd"
   },
   "outputs": [],
   "source": [
    "def testing(sentence):\n",
    "  words = sentence.split()\n",
    "  try:\n",
    "    words= [spell_checker(i) for i in words]\n",
    "    correct_sentence = \" \".join(words)\n",
    "    return correct_sentence\n",
    "  except:\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAPpCs686BiU",
    "outputId": "d86f4fb1-3079-4624-a7a7-cbb38b45213b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humne market jana tha\n",
      "who ghr ke pass ha\n",
      "saarey pakistani shareef hainn\n",
      "poora khadnaaan uthaa kr aa gaya haiii\n",
      "ulta chor kotwal ko date\n",
      "hum lahore ki sarko par awara gardi krre hain\n",
      "islamabad me das daftar hain\n",
      "ki 10 mulk hain\n",
      "mian sb london chalaye gayee\n",
      "adalat ne gayeb krdi ha\n",
      "ho kashmir gye\n",
      "machar pankha se takra gya\n",
      "asdfg qwert asd zxcvb qwert 1234\n",
      "india ke sab say baray network airtel ka hissa baen\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"hmune makret jnaa taha\",\n",
    "             \"whd ghr k pass ha\",\n",
    "             \"saareye pakistaani shareeef haainn\",\n",
    "             \"Poora khadnaaan uthaa kr aa gayay haiiin \",\n",
    "             \"Utla chro kotwal ko datne\",\n",
    "             \"hum lahore ki sarko par awara grdi krre hain\",\n",
    "             \"Islamabd me das dafatr hain\",\n",
    "             \"i 10 mulk hain\",\n",
    "             \"Mian Sb london chalayye gayeye \",\n",
    "             \"Adaalat nep ghyeb krdi ha \",\n",
    "             \"h kahsmir gye\",\n",
    "             \"macher pankhay se takraa gya\",\n",
    "             'asdfg qwert asdf zxcvb qwert 1234', #case where no alternate is available for any word\n",
    "             'idnia ke sba say bray netwrok airtel ka hissaa bnen'] \n",
    "for i in sentences:\n",
    "  print(testing(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "i20-2207",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
